{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08fd45e-25fe-4890-9fec-7950ea8add29",
   "metadata": {},
   "source": [
    "## Model predictions using the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31aeac5c-a8d5-483b-a467-d21eaf3703be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac1bad1-8a84-4700-8a64-072d3bdd4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"C:/Users/Srinidhi/Documents/USF/Data_Science_Programming/week7_assignment/X_train.csv\")\n",
    "X_test = pd.read_csv(\"C:/Users/Srinidhi/Documents/USF/Data_Science_Programming/week7_assignment/X_test.csv\")\n",
    "y_train = pd.read_csv(\"C:/Users/Srinidhi/Documents/USF/Data_Science_Programming/week7_assignment/y_train.csv\")\n",
    "y_test = pd.read_csv(\"C:/Users/Srinidhi/Documents/USF/Data_Science_Programming/week7_assignment/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4760cb-aed3-4744-a284-41d6641376cb",
   "metadata": {},
   "source": [
    "Moving forward I will be choosing F1 score to evaluate the performance of the model. I am choosing this scoring metric because it combines both precision and recall value into one scoring metric by defining the harmonic mean and works best for the classification probelms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09041c0-25dc-44cb-9f0f-640b98f02c4f",
   "metadata": {},
   "source": [
    "# Initial random search based on the parameters set for the Random search for the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c4643d-bfc5-40cf-b98b-e4dd27c62882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "The best f1 score is 0.9779602625902779\n",
      "... with parameters: {'penalty': 'l2', 'C': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = [\n",
    "    { 'penalty': ['l2'], 'C': [ 0.1, 1, 10]}\n",
    "]\n",
    "\n",
    "Lr = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = Lr, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879060e-40a9-4f45-9d02-a0f117db2a9f",
   "metadata": {},
   "source": [
    "## Grid search for the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f621e55-2c69-4092-8203-440dbee8e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.80977723        nan 0.88569615        nan 0.97012285\n",
      "        nan 0.97420683        nan 0.97240533]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.85419407        nan 0.9393384         nan 0.98757345\n",
      "        nan 0.99727065        nan 0.99817974]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9742068308729206\n",
      "... with parameters: {'C': 10, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = [\n",
    "    { 'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "]\n",
    "\n",
    "Lr_grid = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = Lr_grid, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f242c3-ee1f-440d-9ca6-9f2427b7c480",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Random Search for Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff212b1-3d4f-4fba-87e6-63902aac6fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 7 is smaller than n_iter=500. Running 7 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9724053296218781\n",
      "... with parameters: {'kernel': 'linear', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.0001, 0.001, 0.1, 1, 10, 100, 1000], \n",
    "    'kernel': ['linear']\n",
    "}\n",
    "\n",
    "rand_linear_SVC = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = rand_linear_SVC, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797acc4-04db-4234-aaf7-5b694f7ac2fb",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Grid Search for Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91b2ee3b-6ea9-40ca-9287-64a2a2be576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "The best f1 score is 0.9776157863464366\n",
      "... with parameters: {'C': 10, 'kernel': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 10\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.0001, 0.001, 0.1, 1, 10, 100, 1000], \n",
    "    'kernel': ['linear']\n",
    "}\n",
    "\n",
    "Grid_Linear_SVC = SVC()\n",
    "grid_search = GridSearchCV(estimator = Grid_Linear_SVC, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22d704-b443-4fe9-bb2a-c7de16e45d49",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Random Search for Poly kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93f4c25-6d20-417b-8f8f-28d0cbd60b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=500. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9736799574694313\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 1, 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 10\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10, 100], \n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "Random_Poly_SVC = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = Random_Poly_SVC, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610dac6-730f-4918-a85c-e7b3a5cb39ee",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Grid Search for Poly kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968bffb8-9bab-470a-8472-32df9b83b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9724053296218781\n",
      "... with parameters: {'C': 0.1, 'gamma': 1, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10, 100], \n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "\n",
    "Grid_Poly_SVC = SVC()\n",
    "grid_search = GridSearchCV(estimator = Grid_Poly_SVC, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7047c2-5ee2-4bb2-81f4-53927b3c6318",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Random Search for RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947a14d1-01c1-4a6a-a9cc-5fc7a0a01b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 9 is smaller than n_iter=500. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9757976045282547\n",
      "... with parameters: {'kernel': 'rbf', 'gamma': 0.1, 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 10\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10], \n",
    "    'gamma': [1,0.1,0.011],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "Random_rbf_SVC = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = Random_rbf_SVC, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc28099-a15f-41ae-a3dd-bc0b9340f58a",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Grid Search for rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a1069c-2dae-4893-be38-5a48c413c99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "25 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.9601933  0.9601933  0.9601933  0.9601933         nan 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      "        nan 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.98036633 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933         nan 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933         nan 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.96222476\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.98036633\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9621563\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.97299714 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.98728732 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9621563  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933         nan\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.95842426 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.98371445 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96418775 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.96559657 0.95603428 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.95603428 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.97496034 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.96048848\n",
      " 0.96450111 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.95845702 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.95842426 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.97496034 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.97299714 0.96559657 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.96048848]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.96356898 0.96356898 0.96356898 0.96356898        nan 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      "        nan 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.98695939 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898        nan 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898        nan 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96929721\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96754717\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.98695939\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96597514\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.97917725 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.99450583 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96597514 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898        nan\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96559346 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.98670586 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.97079877 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96933257 0.96434208 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96434208 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.980793   0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.98558138\n",
      " 0.97235403 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.97393219 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96559346 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.980793   0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.98318501 0.96846021 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.98558138]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9872873200854851\n",
      "... with parameters: {'min_samples_split': 6, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.004600000000000001, 'max_leaf_nodes': 78, 'max_depth': 14, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08463c0a-da11-441b-bad5-4363f1cdbd50",
   "metadata": {},
   "source": [
    "## Initial random search based on the parameters set for the Random search for the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74b2f9ad-dc89-4133-9870-7661569cc6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.9601933  0.9601933  0.9601933         nan 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.95842426 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.98554901 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.96222476 0.9601933  0.98554901 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.96559657 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.95845702\n",
      " 0.9601933  0.9601933  0.97427671 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.96196021 0.9601933  0.9601933  0.96914417 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.96720388 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96422959 0.9601933\n",
      " 0.9601933  0.95845702 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.96559657 0.9601933  0.9601933  0.9601933  0.98210464 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96623856 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.95603428\n",
      " 0.9601933  0.97451057 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933         nan 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.98036633 0.9601933\n",
      " 0.9601933  0.9601933  0.96819785 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.96050666\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.95845702 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.97669389 0.96559657 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.98912218 0.96450111 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.97299714 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96773322 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933         nan 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.96200737 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.97672997 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.96929585\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.97669389\n",
      " 0.9601933  0.9601933  0.97988232 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.95842426 0.9709138         nan 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.96200737 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.95845702 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.96450111 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.96929585 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.96914417 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.95845702\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.95842426 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933 ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.96356898 0.96356898 0.96356898        nan 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96559346 0.96356898\n",
      " 0.96356898 0.96732387 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.98822276 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96647842 0.96356898 0.96356898\n",
      " 0.96356898 0.96754717 0.96356898 0.9873664  0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96846021 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.97393219\n",
      " 0.96356898 0.96356898 0.97901798 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96624565 0.96356898 0.96356898 0.9789498  0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96647842 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.97044712 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.97543535 0.96356898\n",
      " 0.96356898 0.9724582  0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96846021 0.96356898 0.96356898 0.96356898 0.9904943  0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.97404474 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96647842 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96434208\n",
      " 0.96356898 0.99545038 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898        nan 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.98695939 0.96356898\n",
      " 0.96356898 0.96356898 0.97289131 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96887912\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96604864 0.96356898 0.96356898 0.96356898\n",
      " 0.9748009  0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.98148468 0.96846021 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96732387 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.99183869 0.97279147 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96604864\n",
      " 0.96356898 0.97917725 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.97894553 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96647842 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898        nan 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96647842 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.97436341 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96647842\n",
      " 0.98301146 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.97591625\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.98235761\n",
      " 0.96356898 0.96356898 0.98451387 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96559346 0.98116826        nan 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.97436341 0.96356898 0.96647842 0.96356898 0.96356898\n",
      " 0.9724582  0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.97235403 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.97902531 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.9789498  0.96356898 0.96356898 0.96356898 0.96780612 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.97393219\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96559346 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9891221824708063\n",
      "... with parameters: {'min_samples_split': 13, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0091, 'max_leaf_nodes': 11, 'max_depth': 33, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60cda4-30f0-4578-9a23-e0c9a5c7e13c",
   "metadata": {},
   "source": [
    "## Exhaustive grid search based on the parameters set for the Grid search for the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05ddd05-ec20-4fd1-829b-e5f2429a5f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3000 candidates, totalling 15000 fits\n",
      "The best f1 score is 0.9891221824708063\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 30, 'max_leaf_nodes': 8, 'min_impurity_decrease': 0.0089, 'min_samples_leaf': 3, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(10,15),  \n",
    "    'min_samples_leaf': np.arange(3,7),\n",
    "    'min_impurity_decrease': np.arange(0.0089, 0.0095, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(8,13), \n",
    "    'max_depth': np.arange(30,35), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree_grid = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree_grid, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f1ddf-cc47-44c1-a671-91a719ff1810",
   "metadata": {},
   "source": [
    "## Inference:\n",
    "\n",
    "1. The f1 score for Random Search on Logistic Regression is 0.9762289562289563\n",
    "\n",
    "2. The f1 score for grid search on the logistic regression is 0.9742068308729206\n",
    "\n",
    "3. The f1 score for Random Search on Linear SVM is 0.9724053296218781\n",
    "\n",
    "4. The f1 score for Grid Search on Linear SVM is 0.9776157863464366\n",
    "\n",
    "5. The f1 score for Random Search on Poly SVM is 0.9736799574694313\n",
    "\n",
    "6. The f1 score for Grid Search on Poly SVM is 0.9724053296218781\n",
    "\n",
    "7. The f1 score for Random Search on RBF SVM is 0.9757976045282547\n",
    "\n",
    "8. The f1 score for Grid Search on RBF SVM is 0.9872873200854851\n",
    "\n",
    "9. The f1 score for Random Search on Decision tree classifier is 0.9891221824708063\n",
    "\n",
    "10. 9. The f1 score for exhaustive Grid Search on Decision tree classifier is 0.9891221824708063\n",
    "\n",
    "Based on the above results, it can be clearly seen that when we compare all the f1 score metrics with the models while using both the random search and the grid search to test the range of best parameters, we see that the F1 score is highest for the Decision Tree classifier with almost same testing score. The f1 score for the decision tree classifier using the best parameter tuning using the random search testing and the grid search testing is 98.91. Hence it can be concluded that this is the best performing model out of the rest. SVM RBF appears to be the second best performing model based on the f1 score of 98.72 for the Grid Search test and 97.57 for the Random search test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
